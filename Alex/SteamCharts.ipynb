{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping SteamCharts\n",
    "\n",
    "<a href=\"https://steamcharts.com/\">SteamCharts</a> is a website containing ongoing analysis of Steam's concurrent players. \n",
    "\n",
    "## What We'll Accomplish in this Notebook\n",
    "\n",
    "In this notebook we'll do the following:\n",
    "- Use `BeautifulSoup` to scrape data from SteamCharts\n",
    "- Scrape the TopGames pages, and save them in TopGames.csv\n",
    "- Scrape each game's data page, and save them all in GamesData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import base packages we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the packages for scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we focus on the <a href=\"https://steamcharts.com/top\">Top Games</a> pages.\n",
    "\n",
    "We begin with some testing on the front page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's make a soup object\n",
    "url = \"https://steamcharts.com/top\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text,'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe to store all the infos for the last 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>App_id</th>\n",
       "      <th>Current Players</th>\n",
       "      <th>Peak Players</th>\n",
       "      <th>Hours Played</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ranking</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Counter-Strike: Global Offensive</td>\n",
       "      <td>730</td>\n",
       "      <td>468446</td>\n",
       "      <td>1119102</td>\n",
       "      <td>511641971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dota 2</td>\n",
       "      <td>570</td>\n",
       "      <td>322235</td>\n",
       "      <td>678417</td>\n",
       "      <td>299451762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLAYERUNKNOWN'S BATTLEGROUNDS</td>\n",
       "      <td>578080</td>\n",
       "      <td>193342</td>\n",
       "      <td>418384</td>\n",
       "      <td>131122089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apex Legends</td>\n",
       "      <td>1172470</td>\n",
       "      <td>132505</td>\n",
       "      <td>330879</td>\n",
       "      <td>107693951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Team Fortress 2</td>\n",
       "      <td>440</td>\n",
       "      <td>81521</td>\n",
       "      <td>109540</td>\n",
       "      <td>60877771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Name   App_id  Current Players  \\\n",
       "Ranking                                                               \n",
       "0        Counter-Strike: Global Offensive      730           468446   \n",
       "1                                  Dota 2      570           322235   \n",
       "2           PLAYERUNKNOWN'S BATTLEGROUNDS   578080           193342   \n",
       "3                            Apex Legends  1172470           132505   \n",
       "4                         Team Fortress 2      440            81521   \n",
       "\n",
       "         Peak Players  Hours Played  \n",
       "Ranking                              \n",
       "0             1119102     511641971  \n",
       "1              678417     299451762  \n",
       "2              418384     131122089  \n",
       "3              330879     107693951  \n",
       "4              109540      60877771  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We scrape the entire table and put it into a dataframe\n",
    "topgames = soup.find('table',{'class':\"common-table\"} )\n",
    "df_0 = pd.read_html(str(topgames))[0]\n",
    "\n",
    "## Drop Unnamed:0 and Last 30 Days, then rename the index to ranking\n",
    "df_1 = df_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n",
    "df_1.index.name = 'Ranking'\n",
    "\n",
    "## For every entry we find the app_id and store it\n",
    "app_ids = []\n",
    "for game in soup.find_all('td',{'class':\"game-name\"}):\n",
    "    app_id = game.find('a')['href'].replace('/app/','')\n",
    "    app_ids.append(app_id)\n",
    "\n",
    "## Add the app_ids to the dataframe\n",
    "df_1.insert(1,'App_id',app_ids,True)\n",
    "\n",
    "##\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We make the above test into a function, then scrape every page into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function scrapes the table from the given url and appends it to df,\n",
    "## after cleaning the columns a bit and fixing the index\n",
    "\n",
    "## Originially had a flag in case the table is empty, but now it's commented out\n",
    "def scrape_table_0(url,df):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text,'html')\n",
    "    topgames = soup.find('table',{'class':\"common-table\"} )\n",
    "    \n",
    "    ## If there is not table, return 0\n",
    "    ##if topgames == 'None':\n",
    "    ##    return(df,0)\n",
    "    \n",
    "    df_tmp_0 = pd.read_html(str(topgames))[0]\n",
    "    df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n",
    "    \n",
    "    app_ids = []\n",
    "    for game in soup.find_all('td',{'class':\"game-name\"}):\n",
    "        app_id = game.find('a')['href'].replace('/app/','')\n",
    "        app_ids.append(app_id)\n",
    "    df_tmp_1.insert(1,'App_id',app_ids,True)\n",
    "    \n",
    "    df = df.append(df_tmp_1,ignore_index=True)\n",
    "    df.index.name = 'Ranking'\n",
    "    ##return(df,1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "page = 1\n",
    "url = \"https://steamcharts.com/top/p.\" + str(page)\n",
    "##test,flag = scrape_table(url,df_1)\n",
    "test = scrape_table_0(url,df_1)\n",
    "##print(flag)\n",
    "##test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the main loop (watch out it takes time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The max page gets constantly updated, so we create a function to find it\n",
    "def find_max_page():\n",
    "    flag = 1\n",
    "    ## It's generally above 500, so this saves some time\n",
    "    page = 500\n",
    "    while(flag):\n",
    "        url = \"https://steamcharts.com/top/p.\" + str(page)\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.text,'html')\n",
    "        header = soup.find('h1').text\n",
    "        if header == 'Page Not Found':\n",
    "            flag = 0\n",
    "            page = page-1\n",
    "        else:\n",
    "            page = page+1\n",
    "    ## We take 10 away as the last few pages are always changing\n",
    "    return(page-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "## Loop through all pages\n",
    "## The first page is already in df_1\n",
    "page = 2   \n",
    "##max_page = find_max_page()\n",
    "\n",
    "## Instead of struggling with finding the max page, we know it's always bigger than 500.\n",
    "## The last pages are not informative anyway.\n",
    "max_page = 500\n",
    "\n",
    "## flag = 1\n",
    "## We use df_1 to start the loop\n",
    "df_old = df_1.copy()\n",
    "\n",
    "for i in range (0,max_page):\n",
    "    url = \"https://steamcharts.com/top/p.\" + str(page)\n",
    "    ##df_new,flag = scrape_table_0(url,df_old)\n",
    "    df_new = scrape_table_0(url,df_old)\n",
    "    ##if flag == 0:\n",
    "    ##    print('Page p.',page,'is empty.')\n",
    "    ##    df_final = df_new.copy()\n",
    "    ##else:\n",
    "    ##   df_old = df_new.copy()\n",
    "    df_old = df_new.copy()\n",
    "    page = page + 1\n",
    "    if page%50==0:\n",
    "        print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_0 = df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the final datafram to a .csv file\n",
    "df_final_0.to_csv(\"TopGames.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we look at the relvant infos for each game, for example <a href=\"https://steamcharts.com/app/730\">Counter-Strike: Global Offensive</a>.\n",
    "\n",
    "Let's begin with a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First I scrape data from one game\n",
    "app_id = df_final_0.App_id[0]\n",
    "game = df_final_0.loc[df_final_0.App_id == app_id,'Name'].values[0]\n",
    "url = \"https://steamcharts.com/app/\" + app_id\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text,'html')\n",
    "\n",
    "## Put the data in a dataframe and clean it\n",
    "data_tb = soup.find('table',{'class':\"common-table\"} )\n",
    "df_tmp_0 = pd.read_html(str(data_tb))[0]\n",
    "df_tmp_1 = df_tmp_0.rename(columns={'Avg. Players':'Avg_Players','% Gain':'Perc_Gain','Peak Players':'Peak_Players'})\n",
    "##df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n",
    "\n",
    "## I need to change the columns to make the dataframe better\n",
    "cols_0 = ['Name','App_id']\n",
    "cols_1 = np.array(df_tmp_1.Month[0:])\n",
    "for i in range(0,len(cols_1)):\n",
    "    cols_1[i] = cols_1[i].replace(' ','_')\n",
    "cols = np.concatenate([cols_0,np.array(df_tmp_1.columns[1:]),cols_1])\n",
    "\n",
    "## I want to create a df where the columns are the months, and the other columns of df_tmp_0\n",
    "df_tmp_2 = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "## I fill in the rows with the info, depending on which of feature of cols[2:6] I am considering\n",
    "if len(df_tmp_2.index) == 0:\n",
    "    new_ind = 0\n",
    "else:\n",
    "    new_ind = df_tmp_2.index[-1] + 1\n",
    "\n",
    "for i in range(0,4):\n",
    "    row = []\n",
    "    row.append(game)\n",
    "    row.append(app_id)\n",
    "    vect = np.zeros(4)\n",
    "    vect[i] = 1\n",
    "    row = row + list(vect)\n",
    "    row = row + list(df_tmp_1.loc[:,cols[i+2]])\n",
    "    df_tmp_2.loc[new_ind] = row\n",
    "    new_ind = new_ind+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the above code into a function that I can run for every game (app_id), and spits out the dataframe to append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_1(app_id):\n",
    "    game = df_final_0.loc[df_final_0.App_id == app_id,'Name'].values[0]\n",
    "    url = \"https://steamcharts.com/app/\" + app_id\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text,'html')\n",
    "\n",
    "    ## Put the data in a dataframe and clean it\n",
    "    data_tb = soup.find('table',{'class':\"common-table\"} )\n",
    "    df_tmp_0 = pd.read_html(str(data_tb))[0]\n",
    "    df_tmp_1 = df_tmp_0.rename(columns={'Avg. Players':'Avg_Players','% Gain':'Perc_Gain','Peak Players':'Peak_Players'})\n",
    "    ##df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n",
    "\n",
    "    ## I need to change the columns to make the dataframe better\n",
    "    cols_0 = ['Name','App_id']\n",
    "    cols_1 = np.array(df_tmp_1.Month[0:])\n",
    "    for i in range(0,len(cols_1)):\n",
    "        cols_1[i] = cols_1[i].replace(' ','_')\n",
    "    cols = np.concatenate([cols_0,np.array(df_tmp_1.columns[1:]),cols_1])\n",
    "\n",
    "    ## I want to create a df where the columns are the months, and the other columns of df_tmp_0\n",
    "    df_tmp_2 = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "    ## I fill in the rows with the info, depending on which of feature of cols[2:6] I am considering\n",
    "    if len(df_tmp_2.index) == 0:\n",
    "        new_ind = 0\n",
    "    else:\n",
    "        new_ind = df_tmp_2.index[-1] + 1\n",
    "\n",
    "    for i in range(0,4):\n",
    "        row = []\n",
    "        row.append(game)\n",
    "        row.append(app_id)\n",
    "        vect = np.zeros(4)\n",
    "        vect[i] = 1\n",
    "        row = row + list(vect)\n",
    "        row = row + list(df_tmp_1.loc[:,cols[i+2]])\n",
    "        df_tmp_2.loc[new_ind] = row\n",
    "        new_ind = new_ind+1\n",
    "    return(df_tmp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the main loop (watch out it takes time!)\n",
    "One should notice that most of the later games are not very popular, so it might be better to just cut them off, as they probably don't hold relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 12525\n",
      "Count = 0\n",
      "Count = 50\n",
      "Count = 100\n",
      "Count = 150\n",
      "Count = 200\n",
      "Count = 250\n",
      "Count = 300\n",
      "Count = 350\n",
      "Count = 400\n",
      "Count = 450\n",
      "Count = 500\n",
      "Count = 550\n",
      "Count = 600\n",
      "Count = 650\n",
      "Count = 700\n",
      "Count = 750\n",
      "Count = 800\n",
      "Count = 850\n",
      "Count = 900\n",
      "Count = 950\n",
      "Count = 1000\n",
      "Count = 1050\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-604-a49ab736ceb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mapp_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_final_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApp_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_final_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_final_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApp_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mapp_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_table_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf_tmp_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-602-9b685d48bbd5>\u001b[0m in \u001b[0;36mscrape_table_1\u001b[0;34m(app_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m## Put the data in a dataframe and clean it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"common-table\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_tmp_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf_tmp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Avg. Players'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Avg_Players'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'% Gain'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Perc_Gain'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Peak Players'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Peak_Players'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m##df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         )\n\u001b[1;32m   1085\u001b[0m     \u001b[0mvalidate_header_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m     return _parse(\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0mio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mretained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;31m# if `io` is an io-like object, check if it's seekable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mparse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfooter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtuples\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \"\"\"\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_thead_tbody_tfoot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse_tables\u001b[0;34m(self, doc, match, attrs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No tables found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "## Loop through all app_id\n",
    "df_tmp_3 = df_tmp_2.copy()\n",
    "print('Total =', len(df_final_0.App_id))\n",
    "count = 0\n",
    "## for app_id in df_final_0.App_id[1:10]:\n",
    "for app_id in df_final_0.App_id:\n",
    "    game = df_final_0.loc[df_final_0.App_id == app_id,'Name'].values[0]\n",
    "    test = scrape_table_1(app_id)\n",
    "    df_tmp_3 = df_tmp_3.append(test,ignore_index=True)\n",
    "    if count%50==0:\n",
    "        print('Count =',count)\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_1 = df_tmp_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the final datafram to a .csv file\n",
    "df_final_1.to_csv(\"GamesData.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally let's try to scrape the store of steam: store.steampowered.com.\n",
    "For example <a href=\"https://store.steampowered.com/app/730/CounterStrike_Global_Offensive/\">Counter-Strike: Global Offensive</a>.\n",
    "\n",
    "Let's begin with a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First I scrape data from one game\n",
    "app_id = df_final_0.App_id[0]\n",
    "game = df_final_0.loc[df_final_0.App_id == app_id,'Name'].values[0]\n",
    "url = \"https://store.steampowered.com/app/\" + app_id\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text,'html')\n",
    "\n",
    "## Put the data in a dataframe and clean it\n",
    "data_tb = soup.find('table',{'class':\"common-table\"} )\n",
    "df_tmp_0 = pd.read_html(str(data_tb))[0]\n",
    "df_tmp_1 = df_tmp_0.rename(columns={'Avg. Players':'Avg_Players','% Gain':'Perc_Gain','Peak Players':'Peak_Players'})\n",
    "##df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First I scrape data from one game\n",
    "app_id = df_final_0.App_id[0]\n",
    "game = df_final_0.loc[df_final_0.App_id == app_id,'Name'].values[0]\n",
    "url = \"https://steamcharts.com/app/\" + app_id\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text,'html')\n",
    "\n",
    "## Put the data in a dataframe and clean it\n",
    "data_tb = soup.find('table',{'class':\"common-table\"} )\n",
    "df_tmp_0 = pd.read_html(str(data_tb))[0]\n",
    "df_tmp_1 = df_tmp_0.rename(columns={'Avg. Players':'Avg_Players','% Gain':'Perc_Gain','Peak Players':'Peak_Players'})\n",
    "##df_tmp_1 = df_tmp_0.drop(columns=['Unnamed: 0','Last 30 Days'])\n",
    "\n",
    "## I need to change the columns to make the dataframe better\n",
    "cols_0 = ['Name','App_id']\n",
    "cols_1 = np.array(df_tmp_1.Month[0:])\n",
    "for i in range(0,len(cols_1)):\n",
    "    cols_1[i] = cols_1[i].replace(' ','_')\n",
    "cols = np.concatenate([cols_0,np.array(df_tmp_1.columns[1:]),cols_1])\n",
    "\n",
    "## I want to create a df where the columns are the months, and the other columns of df_tmp_0\n",
    "df_tmp_2 = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "## I fill in the rows with the info, depending on which of feature of cols[2:6] I am considering\n",
    "if len(df_tmp_2.index) == 0:\n",
    "    new_ind = 0\n",
    "else:\n",
    "    new_ind = df_tmp_2.index[-1] + 1\n",
    "\n",
    "for i in range(0,4):\n",
    "    row = []\n",
    "    row.append(game)\n",
    "    row.append(app_id)\n",
    "    vect = np.zeros(4)\n",
    "    vect[i] = 1\n",
    "    row = row + list(vect)\n",
    "    row = row + list(df_tmp_1.loc[:,cols[i+2]])\n",
    "    df_tmp_2.loc[new_ind] = row\n",
    "    new_ind = new_ind+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
