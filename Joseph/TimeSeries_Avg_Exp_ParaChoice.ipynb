{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the SimpleExpSmoothing object\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "# Import the ARIMA object\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time series data:\n",
    "timedata = pd.read_csv('SteamCharts.csv',low_memory=False)\n",
    "\n",
    "# Replace inf to NaN:\n",
    "timedata = timedata.replace({np.inf:np.nan})\n",
    "timedata = timedata.replace({'+Inf':np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Avg_Players.\n",
    "avg_players = timedata.loc[timedata['Avg_Players']==1].reset_index(drop=True)\n",
    "avg_pure = avg_players.drop(columns=['App_id', 'Name', 'Avg_Players', 'Gain', 'Perc_Gain', 'Peak_Players',\n",
    "       'Last 30 Days']).astype(float)\n",
    "\n",
    "avg_players['max'] = avg_pure.max(axis=1)\n",
    "avg_pure = avg_pure.drop(avg_players.loc[avg_players['max']<=0].index).drop(avg_players.loc[avg_players['max'].isna()==True].index)\n",
    "avg_pure = avg_pure.reset_index(drop=True)\n",
    "\n",
    "avg_players = avg_players.drop(avg_players.loc[avg_players['max']<=0].index).drop(avg_players.loc[avg_players['max'].isna()==True].index)\n",
    "avg_players = avg_players.reset_index(drop=True)\n",
    "\n",
    "birth = [avg_pure.iloc[i].last_valid_index() for i in range(len(avg_pure))]\n",
    "avg_players['birth']=birth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select games with the max of avg_players larger than or equal to 10 and are created between July 2012 and March 2021. There are 6471 such games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = avg_players.loc[avg_players['max']>=10]\n",
    "avg_pure1 = avg_pure.loc[avg_players['max']>=10]\n",
    "\n",
    "avg1 = avg1.loc[avg_players['birth']!='July 2012']\n",
    "avg1 = avg1.loc[avg_players['birth']!='March 2021']\n",
    "avg1 = avg1.loc[avg_players['birth']!='April 2021'].reset_index(drop=True)\n",
    "\n",
    "avg_pure1 = avg_pure1.loc[avg_players['birth']!='July 2012']\n",
    "avg_pure1 = avg_pure1.loc[avg_players['birth']!='March 2021']\n",
    "avg_pure1 = avg_pure1.loc[avg_players['birth']!='April 2021'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select games with no NaN data in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomplete_data = []\n",
    "for i in range(len(avg1)):\n",
    "    temp = avg_pure1.iloc[i]\n",
    "    if np.isnan(temp[temp.first_valid_index():temp.last_valid_index()]).any():\n",
    "        incomplete_data.append(i)\n",
    "len(incomplete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg2 = avg1.drop(incomplete_data).reset_index(drop=True)\n",
    "avg_pure2 = avg_pure1.drop(incomplete_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus our attention on predicting 6 horizon given 12 month datas of games we want to predict.\n",
    "\n",
    "Note that different horizon or given data may change the accuracy or the best smoothing/metric for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping datas with less than 18 months\n",
    "X = avg_pure2.copy()\n",
    "list1 = []\n",
    "for i in range(len(X)):\n",
    "    if len(X.iloc[i][X.iloc[i].first_valid_index():X.iloc[i].last_valid_index()])<18:\n",
    "        list1.append(i)\n",
    "X=X.drop(index = list1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "test_index = random.sample(range(len(X)),int(len(X)/4))\n",
    "X_test = X.iloc[test_index]\n",
    "\n",
    "train_index = list(range(len(X)))\n",
    "for i in test_index:\n",
    "    train_index.remove(i)\n",
    "X_train = X.iloc[train_index].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>April 2021</th>\n",
       "      <th>March 2021</th>\n",
       "      <th>February 2021</th>\n",
       "      <th>January 2021</th>\n",
       "      <th>December 2020</th>\n",
       "      <th>November 2020</th>\n",
       "      <th>October 2020</th>\n",
       "      <th>September 2020</th>\n",
       "      <th>August 2020</th>\n",
       "      <th>July 2020</th>\n",
       "      <th>...</th>\n",
       "      <th>April 2013</th>\n",
       "      <th>March 2013</th>\n",
       "      <th>February 2013</th>\n",
       "      <th>January 2013</th>\n",
       "      <th>December 2012</th>\n",
       "      <th>November 2012</th>\n",
       "      <th>October 2012</th>\n",
       "      <th>September 2012</th>\n",
       "      <th>August 2012</th>\n",
       "      <th>July 2012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186110.65</td>\n",
       "      <td>193114.18</td>\n",
       "      <td>198957.52</td>\n",
       "      <td>201247.19</td>\n",
       "      <td>189233.58</td>\n",
       "      <td>179520.26</td>\n",
       "      <td>162585.64</td>\n",
       "      <td>169093.71</td>\n",
       "      <td>192492.61</td>\n",
       "      <td>211700.30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45629.18</td>\n",
       "      <td>53350.61</td>\n",
       "      <td>59394.05</td>\n",
       "      <td>44864.92</td>\n",
       "      <td>67000.69</td>\n",
       "      <td>99899.97</td>\n",
       "      <td>50809.40</td>\n",
       "      <td>53312.41</td>\n",
       "      <td>64263.74</td>\n",
       "      <td>68439.75</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77590.25</td>\n",
       "      <td>86369.82</td>\n",
       "      <td>117742.27</td>\n",
       "      <td>142117.25</td>\n",
       "      <td>61171.65</td>\n",
       "      <td>55408.03</td>\n",
       "      <td>48550.80</td>\n",
       "      <td>46430.53</td>\n",
       "      <td>53287.65</td>\n",
       "      <td>58238.72</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46416.38</td>\n",
       "      <td>42951.59</td>\n",
       "      <td>39209.96</td>\n",
       "      <td>36124.14</td>\n",
       "      <td>35990.05</td>\n",
       "      <td>39914.02</td>\n",
       "      <td>41298.79</td>\n",
       "      <td>56330.88</td>\n",
       "      <td>59629.02</td>\n",
       "      <td>39385.15</td>\n",
       "      <td>...</td>\n",
       "      <td>12329.65</td>\n",
       "      <td>1960.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36127.13</td>\n",
       "      <td>38513.38</td>\n",
       "      <td>41441.43</td>\n",
       "      <td>44647.85</td>\n",
       "      <td>40570.33</td>\n",
       "      <td>36057.62</td>\n",
       "      <td>32493.44</td>\n",
       "      <td>33944.49</td>\n",
       "      <td>35429.43</td>\n",
       "      <td>33497.40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>6.39</td>\n",
       "      <td>12.99</td>\n",
       "      <td>15.77</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.70</td>\n",
       "      <td>7.80</td>\n",
       "      <td>12.50</td>\n",
       "      <td>20.95</td>\n",
       "      <td>15.24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.01</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.92</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>3.57</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.54</td>\n",
       "      <td>5.91</td>\n",
       "      <td>5.87</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>1.29</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.34</td>\n",
       "      <td>4.08</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3217 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      April 2021  March 2021  February 2021  January 2021  December 2020  \\\n",
       "0      186110.65   193114.18      198957.52     201247.19      189233.58   \n",
       "1       45629.18    53350.61       59394.05      44864.92       67000.69   \n",
       "2       77590.25    86369.82      117742.27     142117.25       61171.65   \n",
       "3       46416.38    42951.59       39209.96      36124.14       35990.05   \n",
       "4       36127.13    38513.38       41441.43      44647.85       40570.33   \n",
       "...          ...         ...            ...           ...            ...   \n",
       "3212        6.39       12.99          15.77          9.66           9.03   \n",
       "3213        0.80        1.06           1.54          1.24           1.16   \n",
       "3214        3.33        4.01           2.72          2.30           2.45   \n",
       "3215        3.57        4.25           4.73          5.31           5.30   \n",
       "3216        1.29        2.42           1.03          1.05           1.27   \n",
       "\n",
       "      November 2020  October 2020  September 2020  August 2020  July 2020  \\\n",
       "0         179520.26     162585.64       169093.71    192492.61  211700.30   \n",
       "1          99899.97      50809.40        53312.41     64263.74   68439.75   \n",
       "2          55408.03      48550.80        46430.53     53287.65   58238.72   \n",
       "3          39914.02      41298.79        56330.88     59629.02   39385.15   \n",
       "4          36057.62      32493.44        33944.49     35429.43   33497.40   \n",
       "...             ...           ...             ...          ...        ...   \n",
       "3212           9.70          7.80           12.50        20.95      15.24   \n",
       "3213           0.84          1.37            0.76         1.74       2.15   \n",
       "3214           2.91          2.92            3.05         2.79       2.31   \n",
       "3215           5.15          4.75            4.54         5.91       5.87   \n",
       "3216           0.91          1.35            2.42         1.34       4.08   \n",
       "\n",
       "      ...  April 2013  March 2013  February 2013  January 2013  December 2012  \\\n",
       "0     ...         NaN         NaN            NaN           NaN            NaN   \n",
       "1     ...         NaN         NaN            NaN           NaN            NaN   \n",
       "2     ...         NaN         NaN            NaN           NaN            NaN   \n",
       "3     ...    12329.65     1960.13            0.0           0.0            NaN   \n",
       "4     ...         NaN         NaN            NaN           NaN            NaN   \n",
       "...   ...         ...         ...            ...           ...            ...   \n",
       "3212  ...         NaN         NaN            NaN           NaN            NaN   \n",
       "3213  ...         NaN         NaN            NaN           NaN            NaN   \n",
       "3214  ...         NaN         NaN            NaN           NaN            NaN   \n",
       "3215  ...         NaN         NaN            NaN           NaN            NaN   \n",
       "3216  ...         NaN         NaN            NaN           NaN            NaN   \n",
       "\n",
       "      November 2012  October 2012  September 2012  August 2012  July 2012  \n",
       "0               NaN           NaN             NaN          NaN        NaN  \n",
       "1               NaN           NaN             NaN          NaN        NaN  \n",
       "2               NaN           NaN             NaN          NaN        NaN  \n",
       "3               NaN           NaN             NaN          NaN        NaN  \n",
       "4               NaN           NaN             NaN          NaN        NaN  \n",
       "...             ...           ...             ...          ...        ...  \n",
       "3212            NaN           NaN             NaN          NaN        NaN  \n",
       "3213            NaN           NaN             NaN          NaN        NaN  \n",
       "3214            NaN           NaN             NaN          NaN        NaN  \n",
       "3215            NaN           NaN             NaN          NaN        NaN  \n",
       "3216            NaN           NaN             NaN          NaN        NaN  \n",
       "\n",
       "[3217 rows x 106 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared, we write the pieces of our model one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential smoothing:\n",
    "\n",
    "def smooth_values(data, i_game, smoothing_level=.2):\n",
    "\n",
    "    game_data = data.iloc[i_game]\n",
    "    months = pd.to_datetime(game_data.index)\n",
    "    game = pd.DataFrame({'Month':months,'Data': game_data.astype(float)}).sort_values(by=['Month'])\n",
    "    temp = game['Data'][game['Data'].first_valid_index():game['Data'].last_valid_index()]\n",
    "    \n",
    "    # Fit exponential smoothing\n",
    "    ses = SimpleExpSmoothing(temp.values)\n",
    "    fit = ses.fit(smoothing_level=smoothing_level, optimized=False)\n",
    "    return fit.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rootmse(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "def Gauss_weight(a, b, epsilon = 20):\n",
    "    return np.exp(-epsilon*rootmse(a,b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the shape of 2 curves, we want to compute the scalar that makes 2 curves close together by scaling only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_scaler(a,b):\n",
    "    return np.sum(a*b)/np.sum(a**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the weight function and minimizing scalar defined above, we define the weight average function that acts as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt_avg(game, data, metric = Gauss_weight, epsilon = 20, threshold=0.4, horizon = 6):\n",
    "    length = len(game)\n",
    "    if np.max(np.abs(game))!=0:\n",
    "        game_scaled = game / rootmse(game,0)\n",
    "    else:\n",
    "        game_scaled = game\n",
    "    pred = np.zeros(length+horizon)\n",
    "    close_index = np.zeros((len(data),2))\n",
    "    j=0\n",
    "    for i in range(len(data)):\n",
    "        temp = data[i]\n",
    "        if len(temp)>=length+horizon:\n",
    "            if np.max(np.abs(temp[:length]))!=0:\n",
    "                temp_scaled = temp * mini_scaler(temp[:length],game_scaled)\n",
    "            else: temp_scaled = temp\n",
    "            weight=metric(game_scaled,temp_scaled[:length], epsilon = epsilon)\n",
    "            if weight >= threshold:\n",
    "                pred = pred + weight * temp_scaled[:length+horizon]\n",
    "                close_index[j]=[i,weight]\n",
    "                j=j+1\n",
    "    if np.max(np.abs(pred[:length])) !=0:\n",
    "        pred = pred * mini_scaler(pred[:length],game)\n",
    "    close_index = close_index[:j]\n",
    "    close_index = close_index[np.argsort(close_index[:, 1])][::-1]\n",
    "    return pred, close_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the error measurement we want to minimise. Since we used l2 norm, i.e. rootmse, in the above calculation, we also use that here.\n",
    "\n",
    "In order to calculate average 'percentage error' over the test set, we normalize the error by the l2 norm of the test data. Equivalently, we are normalizing the l2 norm of the predicted values and the test data before computing the error.\n",
    "\n",
    "Here percentage error refers to the error relative to the l2 norm of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_error(train, test, threshold = 0.4, epsilon = 20):\n",
    "    error = 0\n",
    "    for i in range(len(test)):\n",
    "        [pred, close_index] = wt_avg(test[i][:12],train, threshold = threshold, epsilon = epsilon, horizon = 6)\n",
    "        error = error + rootmse(pred[12:],test[i][12:18]) / np.sum(test[i][12:18]**2)\n",
    "    error = error / len(test)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation to get the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV 5 fold split\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_best_eps_thres(data):\n",
    "    error = 5\n",
    "    best_eps = 50\n",
    "    best_thres = 0.5\n",
    "    for epsilon in np.arange(5,25,5):\n",
    "        for threshold in np.arange(0.1,0.55,0.1):\n",
    "            temp_error = 0\n",
    "            for train_index, test_index in kfold.split(data):\n",
    "                X_train = [data[index] for index in train_index]\n",
    "                X_test = [data[index] for index in test_index]\n",
    "                temp_error = temp_error + Get_error(X_train, X_test, threshold = threshold, epsilon = epsilon)\n",
    "        if temp_error < error:\n",
    "                error = temp_error\n",
    "                best_eps = epsilon\n",
    "                best_thres = threshold\n",
    "    error = error / 5\n",
    "    return best_eps, best_thres, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all pieces defined, we write our main function to compute average CV error and find the best parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Leung\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ult_result = np.zeros((3,4))\n",
    "i=0\n",
    "for alpha in [0.2, 0.4, 0.6, 0.8]:\n",
    "\n",
    "    # getting the smoothed curve for each game\n",
    "    # Ordered in increasing time order.\n",
    "    X1 = X_train.copy()\n",
    "    smooth_timedata = []\n",
    "    broken_data=[]\n",
    "    for i in range(len(X1)):\n",
    "        try:\n",
    "            temp = smooth_values(X1,i, smoothing_level=alpha)\n",
    "            if np.isnan(temp).any():\n",
    "                broken_data.append(i)\n",
    "            else:\n",
    "                smooth_timedata.append(temp)\n",
    "        except:\n",
    "            broken_data.append(i)\n",
    "    \n",
    "    [best_eps, best_thres, error] = Get_best_eps_thres(smooth_timedata)\n",
    "    ult_result[i]=[alpha, best_eps, best_thres, error]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
